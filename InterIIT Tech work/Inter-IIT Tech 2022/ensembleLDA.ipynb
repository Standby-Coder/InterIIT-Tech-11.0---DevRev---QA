{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8YaYBN6IvRDZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2LL4omiwJWz",
        "outputId": "1729c0a0-747a-4537-b70d-85cf14f6a221"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: numpy 1.21.6\n",
            "Uninstalling numpy-1.21.6:\n",
            "  Would remove:\n",
            "    /usr/local/bin/f2py\n",
            "    /usr/local/bin/f2py3\n",
            "    /usr/local/bin/f2py3.8\n",
            "    /usr/local/lib/python3.8/dist-packages/numpy-1.21.6.dist-info/*\n",
            "    /usr/local/lib/python3.8/dist-packages/numpy.libs/libgfortran-2e0d59d6.so.5.0.0\n",
            "    /usr/local/lib/python3.8/dist-packages/numpy.libs/libopenblasp-r0-2d23e62b.3.17.so\n",
            "    /usr/local/lib/python3.8/dist-packages/numpy.libs/libquadmath-2d0c479f.so.0.0.0\n",
            "    /usr/local/lib/python3.8/dist-packages/numpy/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled numpy-1.21.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.24.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.24.1 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.1\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall numpy \n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hv6_xtWowUm0",
        "outputId": "2c2c7f9b-a3c5-4de0-a7a1-3393a39151ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in c:\\python311\\lib\\site-packages (4.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in c:\\python311\\lib\\site-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in c:\\python311\\lib\\site-packages (from gensim) (1.9.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\python311\\lib\\site-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\python311\\lib\\site-packages (from gensim) (2.0.5)\n",
            "Requirement already satisfied: Cython==0.29.32 in c:\\python311\\lib\\site-packages (from gensim) (0.29.32)\n",
            "Requirement already satisfied: pandas in c:\\python311\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.2)\n",
            "Requirement already satisfied: pyfume in c:\\python311\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\python311\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\python311\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.6)\n",
            "Requirement already satisfied: simpful in c:\\python311\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.9.0)\n",
            "Requirement already satisfied: fst-pso in c:\\python311\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\python311\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
            "Requirement already satisfied: miniful in c:\\python311\\lib\\site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
            "Requirement already satisfied: requests in c:\\python311\\lib\\site-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.28.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\python311\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (1.25.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2022.12.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-NbH54THv8Tc"
      },
      "outputs": [],
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_bRS-I81LS2",
        "outputId": "b387a4dc-c48e-49ce-e13d-fef038efad2e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ckred\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\ckred\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\ckred\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "# import nltk\n",
        "nltk.download('wordnet')\n",
        "# import nltk\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YahFyQ9owGVj"
      },
      "outputs": [],
      "source": [
        "docs = []\n",
        "\n",
        "df=pd.read_csv('train_data.csv')\n",
        "\n",
        "for i in range(len(df['Paragraph'])):\n",
        "    docs.append(df['Paragraph'][i])\n",
        "\n",
        "    #for i in range(df.shape[0]):\n",
        "    #rw=df.iloc[i]\n",
        "    #data.append('Paragraph')\n",
        "\n",
        "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(docs)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBn-Jny_yULF",
        "outputId": "10eccbae-5327-49ea-922c-91c82a345bec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "75055\n"
          ]
        }
      ],
      "source": [
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YV9ajcfd1h0J"
      },
      "outputs": [],
      "source": [
        "# Tokenize the documents.\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Split the documents into tokens.\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "for idx in range(len(docs)):\n",
        "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
        "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
        "\n",
        "# Remove numbers, but not words that contain numbers.\n",
        "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
        "\n",
        "# Remove words that are only one character.\n",
        "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Xf5O9vXq1rs4"
      },
      "outputs": [],
      "source": [
        "# Lemmatize the documents.\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "F1nGLea51yQN"
      },
      "outputs": [],
      "source": [
        "# Compute bigrams.\n",
        "from gensim.models import Phrases\n",
        "\n",
        "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
        "bigram = Phrases(docs, min_count=20)\n",
        "for idx in range(len(docs)):\n",
        "    for token in bigram[docs[idx]]:\n",
        "        if '_' in token:\n",
        "            # Token is a bigram, add to document.\n",
        "            docs[idx].append(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dh9beoKE2R_9"
      },
      "outputs": [],
      "source": [
        "from gensim.corpora import Dictionary\n",
        "\n",
        "# Create a dictionary representation of the documents.\n",
        "dictionary = Dictionary(docs)\n",
        "\n",
        "# Bag-of-words representation of the documents.\n",
        "corpus = [dictionary.doc2bow(doc) for doc in docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qydH719y2jtX",
        "outputId": "a11999c5-cd4e-4f7c-e83a-da3dc55edfcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique tokens: 66709\n",
            "Number of documents: 75055\n"
          ]
        }
      ],
      "source": [
        "print('Number of unique tokens: %d' % len(dictionary))\n",
        "print('Number of documents: %d' % len(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ano33jxZ4AgB"
      },
      "outputs": [],
      "source": [
        "# Train LDA model.\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "# Set training parameters.\n",
        "num_topics = 10\n",
        "chunksize = 2000\n",
        "passes = 20\n",
        "iterations = 400\n",
        "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
        "\n",
        "# Make an index to word dictionary.\n",
        "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
        "id2word = dictionary.id2token\n",
        "\n",
        "model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    chunksize=chunksize,\n",
        "    alpha='auto',\n",
        "    eta='auto',\n",
        "    iterations=iterations,\n",
        "    num_topics=num_topics,\n",
        "    passes=passes,\n",
        "    eval_every=eval_every\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjgsmy9F7iEF",
        "outputId": "2164a230-3ed9-4b15-ce3b-eeef67d78c72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average topic coherence: -4.9259.\n",
            "[([(0.055977095, 'the'),\n",
            "   (0.055244114, 'of'),\n",
            "   (0.04389015, 'to'),\n",
            "   (0.031207014, 'and'),\n",
            "   (0.023090677, 'that'),\n",
            "   (0.019567246, 'in'),\n",
            "   (0.018908763, 'is'),\n",
            "   (0.018788397, 'a'),\n",
            "   (0.014685984, 'or'),\n",
            "   (0.011552846, 'for'),\n",
            "   (0.011112772, 'be'),\n",
            "   (0.010565061, 'are'),\n",
            "   (0.009365342, 'it'),\n",
            "   (0.008209435, 'by'),\n",
            "   (0.0081128245, 'not'),\n",
            "   (0.007768786, 'this'),\n",
            "   (0.0073314523, 'an'),\n",
            "   (0.007137246, 'have'),\n",
            "   (0.0068403846, 'with'),\n",
            "   (0.005657693, 'their')],\n",
            "  -0.9543623576904267),\n",
            " ([(0.16503844, 'the'),\n",
            "   (0.063352145, 'of'),\n",
            "   (0.04762937, 'in'),\n",
            "   (0.040807404, 'and'),\n",
            "   (0.023748862, 'wa'),\n",
            "   (0.023151075, 'to'),\n",
            "   (0.014420027, 'by'),\n",
            "   (0.01294425, 'were'),\n",
            "   (0.009820007, 'with'),\n",
            "   (0.009667606, 'on'),\n",
            "   (0.0087735625, 'from'),\n",
            "   (0.008305214, 'a'),\n",
            "   (0.0068181865, 'it'),\n",
            "   (0.0062941182, 'at'),\n",
            "   (0.006040042, 'had'),\n",
            "   (0.0050693112, 'century'),\n",
            "   (0.004603503, 'first'),\n",
            "   (0.0045343293, 'state'),\n",
            "   (0.0045109536, 'which'),\n",
            "   (0.0044616344, 'after')],\n",
            "  -1.1532105195569213),\n",
            " ([(0.04362506, 'the'),\n",
            "   (0.03537337, 'and'),\n",
            "   (0.020399809, 'in'),\n",
            "   (0.016640645, 'for'),\n",
            "   (0.016403733, 'on'),\n",
            "   (0.015887436, 'with'),\n",
            "   (0.011191559, 'to'),\n",
            "   (0.0092450585, 'a'),\n",
            "   (0.009143929, 'it'),\n",
            "   (0.008011117, 'at'),\n",
            "   (0.00783162, 'game'),\n",
            "   (0.0061420053, 'also'),\n",
            "   (0.0061067324, 'specie'),\n",
            "   (0.0059100646, 'red'),\n",
            "   (0.005282269, 'system'),\n",
            "   (0.005111429, 'which'),\n",
            "   (0.0049861, 'from'),\n",
            "   (0.004970096, 'by'),\n",
            "   (0.0049556615, 'used'),\n",
            "   (0.0047646426, 'an')],\n",
            "  -1.540894538164282),\n",
            " ([(0.061910287, 'and'),\n",
            "   (0.049619053, 'in'),\n",
            "   (0.040925562, 'of'),\n",
            "   (0.035219695, 'the'),\n",
            "   (0.01895491, 'is'),\n",
            "   (0.017414724, 'state'),\n",
            "   (0.015180613, 'ha'),\n",
            "   (0.012759805, 'university'),\n",
            "   (0.012082387, 'for'),\n",
            "   (0.011797499, 'world'),\n",
            "   (0.009626998, 'school'),\n",
            "   (0.008657015, 'million'),\n",
            "   (0.008593023, 'international'),\n",
            "   (0.008418789, 'also'),\n",
            "   (0.008194147, 'united'),\n",
            "   (0.008144791, 'it'),\n",
            "   (0.0078008412, 'area'),\n",
            "   (0.0076766512, 'united_state'),\n",
            "   (0.0068776114, 'are'),\n",
            "   (0.0063946536, 'year')],\n",
            "  -1.7624483168385194),\n",
            " ([(0.055922426, 'the'),\n",
            "   (0.038434878, 'and'),\n",
            "   (0.031421855, 'is'),\n",
            "   (0.02884389, 'of'),\n",
            "   (0.02329002, 'are'),\n",
            "   (0.016407335, 'in'),\n",
            "   (0.014009305, 'from'),\n",
            "   (0.013985475, 'greek'),\n",
            "   (0.008780327, 'animal'),\n",
            "   (0.0075076893, 'bc'),\n",
            "   (0.007479381, 'water'),\n",
            "   (0.007263519, 'armenian'),\n",
            "   (0.0072473716, 'region'),\n",
            "   (0.0069958726, 'which'),\n",
            "   (0.0068336744, 'area'),\n",
            "   (0.006008882, 'infection'),\n",
            "   (0.005937424, 'with'),\n",
            "   (0.005921318, 'bird'),\n",
            "   (0.0057697394, 'most'),\n",
            "   (0.005444798, 'ancient')],\n",
            "  -3.1104193473607045),\n",
            " ([(0.07438638, 'city'),\n",
            "   (0.025587067, 'is'),\n",
            "   (0.020208377, 'league'),\n",
            "   (0.016488068, 'new'),\n",
            "   (0.015371833, 'airport'),\n",
            "   (0.01526191, 'football'),\n",
            "   (0.013412928, 'activity'),\n",
            "   (0.01232717, 'area'),\n",
            "   (0.01099898, 'san_diego'),\n",
            "   (0.010902564, 'primary'),\n",
            "   (0.009383526, 'saint'),\n",
            "   (0.009352778, 'park'),\n",
            "   (0.009019339, 'national'),\n",
            "   (0.008850511, 'museum'),\n",
            "   (0.007718781, 'york'),\n",
            "   (0.0075746337, 'square'),\n",
            "   (0.0072973534, 'new_york'),\n",
            "   (0.007215118, 'premier_league'),\n",
            "   (0.007013537, 'miami'),\n",
            "   (0.006425079, 'are')],\n",
            "  -4.650202162756463),\n",
            " ([(0.19293903, 'his'),\n",
            "   (0.1392425, 'he'),\n",
            "   (0.045809384, 'wa'),\n",
            "   (0.03493307, 'film'),\n",
            "   (0.025249748, 'him'),\n",
            "   (0.022817535, 'her'),\n",
            "   (0.01728901, 'who'),\n",
            "   (0.015530325, 'spielberg'),\n",
            "   (0.014342942, 'she'),\n",
            "   (0.014067923, 'had'),\n",
            "   (0.013370488, 'woman'),\n",
            "   (0.011273405, 'child'),\n",
            "   (0.00958601, 'at'),\n",
            "   (0.009375475, 'born'),\n",
            "   (0.009169891, 'carrier'),\n",
            "   (0.008570051, 'mother'),\n",
            "   (0.0073402976, 'household'),\n",
            "   (0.006860483, 'exercise'),\n",
            "   (0.0068530734, 'achieve'),\n",
            "   (0.006736646, 'terminal')],\n",
            "  -5.35058537622198),\n",
            " ([(0.12483686, 'population'),\n",
            "   (0.04963062, 'season'),\n",
            "   (0.04699179, 'minority'),\n",
            "   (0.04267424, 'american'),\n",
            "   (0.042307533, 'genocide'),\n",
            "   (0.032510865, 'native'),\n",
            "   (0.030858824, 'portuguese'),\n",
            "   (0.029354652, 'african'),\n",
            "   (0.027862962, 'muslim'),\n",
            "   (0.024828339, 'architecture'),\n",
            "   (0.01675245, 'hindu'),\n",
            "   (0.014652422, 'decline'),\n",
            "   (0.012693107, 'non'),\n",
            "   (0.011610852, 'hispanic'),\n",
            "   (0.010269963, 'roughly'),\n",
            "   (0.010017125, 'density'),\n",
            "   (0.009556296, 'segment'),\n",
            "   (0.009027752, 'native_american'),\n",
            "   (0.008535796, 'racial'),\n",
            "   (0.007574509, 'ethnic_group')],\n",
            "  -6.88933502008158),\n",
            " ([(0.077477224, 'church'),\n",
            "   (0.044244077, 'christian'),\n",
            "   (0.038088657, 'mosaic'),\n",
            "   (0.035070285, 'witness'),\n",
            "   (0.023510851, 'jehovah'),\n",
            "   (0.023416622, 'dame'),\n",
            "   (0.022988686, 'notre'),\n",
            "   (0.022988686, 'notre_dame'),\n",
            "   (0.02049342, 'jehovah_witness'),\n",
            "   (0.01832571, 'speed'),\n",
            "   (0.017633019, 'christ'),\n",
            "   (0.017218519, 'bible'),\n",
            "   (0.016181119, 'jewish'),\n",
            "   (0.012772249, 'pope'),\n",
            "   (0.012590168, 'congregation'),\n",
            "   (0.01240064, 'jesus'),\n",
            "   (0.011721653, 'cathedral'),\n",
            "   (0.009990095, 'wave'),\n",
            "   (0.009158304, 'worship'),\n",
            "   (0.008955357, 'orthodox')],\n",
            "  -9.022759461784059),\n",
            " ([(1.4990479e-05, 'sunstein'),\n",
            "   (1.4990479e-05, 'cordage'),\n",
            "   (1.4990479e-05, 'unpeopled'),\n",
            "   (1.4990479e-05, 'marxian'),\n",
            "   (1.4990479e-05, 'skinner'),\n",
            "   (1.4990479e-05, 'resourced'),\n",
            "   (1.4990479e-05, 'pettit'),\n",
            "   (1.4990479e-05, 'objectionable'),\n",
            "   (1.4990479e-05, 'nussbaum'),\n",
            "   (1.4990479e-05, 'anyother'),\n",
            "   (1.4990479e-05, 'walzer'),\n",
            "   (1.4990479e-05, 'sandel'),\n",
            "   (1.4990479e-05, 'macintyre'),\n",
            "   (1.4990479e-05, 'communitarians'),\n",
            "   (1.4990479e-05, 'communitarian'),\n",
            "   (1.4990479e-05, 'alasdair'),\n",
            "   (1.4990479e-05, 'structuralist'),\n",
            "   (1.4990479e-05, 'ailen'),\n",
            "   (1.4990479e-05, 'pitcairn_island'),\n",
            "   (1.4990479e-05, 'ˈnɔːrfək')],\n",
            "  -14.82490328816443)]\n"
          ]
        }
      ],
      "source": [
        "top_topics = model.top_topics(corpus)\n",
        "\n",
        "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
        "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
        "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
        "\n",
        "from pprint import pprint\n",
        "pprint(top_topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvY8aPSS2orX",
        "outputId": "574aac96-f80c-4511-d6e2-5f5ecd258b43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Saved\n"
          ]
        }
      ],
      "source": [
        "#FULL\n",
        "\n",
        "#max_epochs = 1\n",
        "#vec_size = 728\n",
        "#alpha = 0.025\n",
        "\n",
        "\n",
        "model_full = gensim.models.ldamodel.LdaModel(corpus, num_topics=50, alpha='auto', eval_every=5)\n",
        "\n",
        "model_full.save(\"ldam.model\")\n",
        "print(\"Model Saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gensim.downloader as api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Python311\\\\Lib\\\\site-packages\\\\gensim'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gensim.__path__[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gensim.models.ensemblelda.EnsembleLda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "SOdtPq3F8Gc1",
        "outputId": "ab07ebdf-8bd7-4b43-e912-cf847b34dc18"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "type object 'LdaModel' has no attribute 'TaggedDocument'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m                 \u001b[39m# For training data, add tags\u001b[39;00m\n\u001b[0;32m     15\u001b[0m                 \u001b[39myield\u001b[39;00m gensim\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mldamodel\u001b[39m.\u001b[39mLdaModel\u001b[39m.\u001b[39mTaggedDocument(tokens, [i])\n\u001b[1;32m---> 17\u001b[0m train_corpus \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(read_corpus(lda_train_file))\n\u001b[0;32m     18\u001b[0m test_corpus \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(read_corpus(lda_test_file, tokens_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[0;32m     19\u001b[0m model_pre \u001b[39m=\u001b[39m gensim\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mldamodel\u001b[39m.\u001b[39mLdaModel(corpus, num_topics\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, alpha\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m, eval_every\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n",
            "Cell \u001b[1;32mIn[19], line 15\u001b[0m, in \u001b[0;36mread_corpus\u001b[1;34m(fname, tokens_only)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[39myield\u001b[39;00m tokens\n\u001b[0;32m     13\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     \u001b[39m# For training data, add tags\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     \u001b[39myield\u001b[39;00m gensim\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mldamodel\u001b[39m.\u001b[39;49mLdaModel\u001b[39m.\u001b[39;49mTaggedDocument(tokens, [i])\n",
            "\u001b[1;31mAttributeError\u001b[0m: type object 'LdaModel' has no attribute 'TaggedDocument'"
          ]
        }
      ],
      "source": [
        "#Pretrained\n",
        "test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\n",
        "lda_train_file = os.path.join(test_data_dir, 'ensemblelda')\n",
        "lda_test_file = os.path.join(test_data_dir, 'ensemblelda')\n",
        "import smart_open\n",
        "\n",
        "def read_corpus(fname, tokens_only=False):\n",
        "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            tokens = gensim.utils.simple_preprocess(line)\n",
        "            if tokens_only:\n",
        "                yield tokens\n",
        "            else:\n",
        "                # For training data, add tags\n",
        "                yield gensim.models.ldamodel.LdaModel.TaggedDocument(tokens, [i])\n",
        "\n",
        "train_corpus = list(read_corpus(lda_train_file))\n",
        "test_corpus = list(read_corpus(lda_test_file, tokens_only=True))\n",
        "model_pre = gensim.models.ldamodel.LdaModel(corpus, num_topics=50, alpha='auto', eval_every=5)\n",
        "model_pre.train(train_corpus, total_examples=model_pre.corpus_count, epochs=model_pre.epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnRA_rFGVLBc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
